import Chat from "../db/models/chat.model.js";
import Message from "../db/models/message.model.js";
import { AuthHandler } from "../helpers/AuthHandler.js";
import { service, streamHandler } from "../helpers/decorators.js";
import { Logger } from "../helpers/Logger.js";
import { StreamContext } from "../helpers/StreamManager.js";

interface ChatStreamData {
  chatId: number;
  message: string;
  attachments?: Array<{
    filename: string;
    type: string;
    data: string; // base64 encoded data
  }>;
}

interface ChatResponseChunk_MessageStart {
  type: 'message_start';
  data: {
    chatId: number;
    agentMessageId: number;
    userMessageId: number;
    userMessage: {
      content: string;
    }
  }
}

interface ChatResponseChunk_ContentChunk {
  type: 'content_chunk';
  data: {
    chunk: string;
    fullContent: string;
    agentMessageId: number;
    userMessageId: number;
  };
}

interface ChatResponseChunk_ContentComplete {
  type: 'content_complete';
  data: {
    agentMessageId: number;
    userMessageId: number;
  }
}

interface ChatResponseChunk_Error {
  type: 'error';
  data: {
    error: string;
    chatId?: number;
    userMessageId: number;
    agentMessageId: number;
  };
}

type ChatResponseChunk = ChatResponseChunk_MessageStart | ChatResponseChunk_ContentChunk | ChatResponseChunk_ContentComplete | ChatResponseChunk_Error;

@service
export class ChatStreamService {

  @streamHandler('chat.stream')
  async handleChatStream(data: ChatStreamData, stream: StreamContext): Promise<void> {
    try {
      const { chatId, message } = data;
      const chat = await Chat.findOne({
        where: { id: chatId, userId: AuthHandler.profile?.sub },
      })

      if (!chat) {
        stream.error('Chat not found with chatId: ' + chatId);
      }

      // Validate the message content
      if (!message || message.trim() === '') {
        stream.error('Message cannot be empty');
        return;
      }

      // if there was pending agent message, update its status to 'error'
      // this could happen if user quits electron while the agent is still processing the message
      const lastAgentMessage = await Message.findOne({
        where: {
          chatId: chatId,
          type: 'assistant',
          status: 'pending',
        },
        order: [['createdAt', 'DESC']],
      });
      if (lastAgentMessage) {
        await lastAgentMessage.update({
          status: 'error'
        })
      }

      // Create a new message in the database
      const userMessage = (await Message.create({
        chatId: chatId,
        type: 'user',
        content: message,
        status: 'success',
      })).get({ plain: true });

      // Update the chat's last message timestamp
      await Chat.update({ lastMessageAt: new Date() }, { where: { id: chatId } });

      // Create empty agent message
      const agentMessage = (await Message.create({
        chatId: chatId,
        type: 'assistant',
        content: '',
        status: 'pending',
      })).get({ plain: true });

      // Send message start notification
      stream.write({
        type: 'message_start',
        data: {
          chatId,
          userMessageId: userMessage.id,
          agentMessageId: agentMessage.id,
          userMessage: {
            content: userMessage.content,
          }
        }
      } as ChatResponseChunk);


      // Stream response chunks (simulating AI response)
      const { fullContent } = await this.simulateAIResponse(stream, message, userMessage.id!, agentMessage.id!);

      // Change the agent message status to 'success'
      await Message.update(
        { status: 'success', content: fullContent },
        { where: { id: agentMessage.id } }
      );

      // Complete the stream
      stream.end({
        type: 'content_complete',
        data: {
          agentMessageId: agentMessage.id,
          userMessageId: userMessage.id,
        }
      } as ChatResponseChunk);

    } catch (error) {
      // update last agent message status to 'error'
      await Message.update(
        { status: 'error', content: error instanceof Error ? error.message : 'Unknown error occurred' },
        { where: { type: 'assistant', chatId: data.chatId, status: 'pending' } }
      );

      Logger.error('Error in chat stream:', error);
      stream.error(error instanceof Error ? error.message : 'Unknown error occurred');
    }
  }

  private async simulateAIResponse(stream: StreamContext, userMessage: string, userMessageId: number, agentMessageId: number) {
    // This is a simulation - replace with actual AI/LLM integration
    let responseText = `This is a simulated response to the message: "${userMessage}". It will be streamed in chunks as if it were generated by an AI model.`;
    for (let i = 0; i < 200; i++) {
      responseText += `# Chunk ${i + 1} of the response.\n`;
    }

    const words = responseText.split(' ');
    let accumulatedText = '';

    for (let i = 0; i < words.length; i++) {
      // Simulate network delay
      await new Promise(resolve => setTimeout(resolve, 100 + Math.random() * 200));

      accumulatedText += (i > 0 ? ' ' : '') + words[i];

      if (stream.isStreamEnded()) break;

      // Stream the chunk
      stream.write({
        type: 'content_chunk',
        data: {
          chunk: words[i] + (i < words.length - 1 ? ' ' : ''),
          fullContent: accumulatedText,
          agentMessageId,
          userMessageId
        },
      } as ChatResponseChunk);
    }

    return {
      fullContent: accumulatedText,
    }
  }
}
